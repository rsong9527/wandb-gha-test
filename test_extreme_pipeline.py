#!/usr/bin/env python3
"""
æç«¯Pipelineæµ‹è¯• - å°è¯•å¤ç°Wovençš„å…·ä½“é—®é¢˜
æ¨¡æ‹Ÿæ›´å¤æ‚çš„åœºæ™¯ï¼šå¤§é‡æ–‡ä»¶ã€å¤æ‚çš„artifactç»“æ„ã€å¹¶å‘ä¸Šä¼ ç­‰
"""

import os
import sys
import time
import wandb
import tempfile
import json
import threading
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

def create_complex_pipeline_outputs():
    """åˆ›å»ºå¤æ‚çš„pipelineè¾“å‡ºæ–‡ä»¶ç»“æ„"""
    temp_dir = Path(tempfile.mkdtemp(prefix="complex_pipeline_"))
    
    files = []
    
    # 1. åˆ›å»ºå¤šä¸ªå¤§çš„.pbæ–‡ä»¶ï¼ˆæ¨¡æ‹ŸTritonæ¨¡å‹ï¼‰
    for i in range(3):
        pb_file = temp_dir / f"triton_model_{i:02d}.pb"
        with open(pb_file, 'wb') as f:
            # åˆ›å»ºçº¦2MBçš„æ–‡ä»¶
            chunk = b'\x08\x96\x01\x12\x04\x08\x01\x10\x01' * 1000
            for _ in range(20):  # 20 * 100KB â‰ˆ 2MB
                f.write(chunk)
        files.append(pb_file)
    
    # 2. åˆ›å»ºåµŒå¥—ç›®å½•ç»“æ„ï¼ˆæ¨¡æ‹Ÿå¤æ‚çš„è¾“å‡ºï¼‰
    for dataset in ['demo_day_2023_few_s1', 'multiview_5_cams', 'combined']:
        dataset_dir = temp_dir / dataset
        dataset_dir.mkdir()
        
        # è¯„ä¼°ç»“æœæ–‡ä»¶
        results_file = dataset_dir / f"{dataset}_results.json"
        results_data = {
            "dataset": dataset,
            "metrics": {
                "HOTA": 65.63 + (hash(dataset) % 100) / 100,
                "DetA": 68.05 + (hash(dataset) % 100) / 100,
                "AssA": 63.423 + (hash(dataset) % 100) / 100,
                "DetRe": 83.573,
                "DetPr": 74.746,
                "AssRe": 65.567,
                "AssPr": 91.818,
                "LocA": 89.072,
                "OWTA": 72.796
            },
            "identity_metrics": {
                "IDF1": 70.281,
                "IDR": 74.431,
                "IDP": 66.57,
                "IDTP": 4582,
                "IDFN": 1574,
                "IDFP": 2301
            },
            "count_metrics": {
                "Dets": 6883,
                "GT_Dets": 6156,
                "IDs": 38,
                "GT_IDs": 8
            }
        }
        with open(results_file, 'w') as f:
            json.dump(results_data, f, indent=2)
        files.append(results_file)
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
        for j in range(5):
            output_file = dataset_dir / f"output_{j:03d}.pb"
            with open(output_file, 'wb') as f:
                f.write(b'\x08\x96\x01\x12\x04\x08\x01\x10\x01' * (200 * (j + 1)))
            files.append(output_file)
    
    # 3. åˆ›å»ºé…ç½®å’Œå…ƒæ•°æ®æ–‡ä»¶
    config_file = temp_dir / "pipeline_config.yaml"
    with open(config_file, 'w') as f:
        f.write("""
pipeline:
  version: v1.15.0
  type: demo_day_2023_few_s1
  triton_client: true
  
datasets:
  - demo_day_2023_few_s1
  - multiview_5_cams
  - combined
  
inference:
  batch_size: 32
  max_sequence_length: 512
  model_path: /models/triton_model_00.pb
  
evaluation:
  metrics:
    - HOTA
    - DetA
    - AssA
    - MTMC
  thresholds: [0.5, 0.75, 0.9]
""")
    files.append(config_file)
    
    return temp_dir, files

def create_multiple_artifacts(run, temp_dir, files):
    """åˆ›å»ºå¤šä¸ªä¸åŒç±»å‹çš„artifacts"""
    artifacts = []
    
    # 1. Pipelineè¾“å‡ºartifactï¼ˆä¸»è¦çš„ï¼‰
    pipeline_artifact = wandb.Artifact(
        name="outputs-demo_day_2023_few_s1",
        type="pipeline_output",
        description="å®Œæ•´çš„pipelineè¾“å‡º - æ¨¡æ‹ŸWovenåœºæ™¯"
    )
    
    # æ·»åŠ æ‰€æœ‰æ–‡ä»¶
    for file in files:
        pipeline_artifact.add_file(str(file))
    
    artifacts.append(("pipeline_output", pipeline_artifact))
    
    # 2. æ¨¡å‹artifact
    model_artifact = wandb.Artifact(
        name="triton-models",
        type="model",
        description="Tritonæ¨ç†æ¨¡å‹"
    )
    
    for file in files:
        if file.name.endswith('.pb'):
            model_artifact.add_file(str(file))
    
    artifacts.append(("model", model_artifact))
    
    # 3. è¯„ä¼°ç»“æœartifact
    results_artifact = wandb.Artifact(
        name="evaluation-results",
        type="evaluation",
        description="MTMCè¯„ä¼°ç»“æœ"
    )
    
    for file in files:
        if file.name.endswith('.json'):
            results_artifact.add_file(str(file))
    
    artifacts.append(("evaluation", results_artifact))
    
    return artifacts

def upload_artifact_with_retry(artifact_info, max_retries=3):
    """å¸¦é‡è¯•çš„artifactä¸Šä¼ """
    artifact_type, artifact = artifact_info
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ ä¸Šä¼ {artifact_type} artifact (å°è¯• {attempt + 1}/{max_retries})")
            logged_artifact = wandb.log_artifact(artifact)
            logged_artifact.wait()
            print(f"âœ… {artifact_type} artifactä¸Šä¼ æˆåŠŸ: {logged_artifact.name}:{logged_artifact.version}")
            return logged_artifact
        except Exception as e:
            print(f"âŒ {artifact_type} artifactä¸Šä¼ å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•

def main():
    print("=== æç«¯Pipelineæµ‹è¯• ===")
    
    # æ£€æŸ¥ç¯å¢ƒ
    api_key = os.getenv('WANDB_API_KEY')
    base_url = os.getenv('WANDB_BASE_URL', 'https://api.wandb.ai')
    is_ci = os.getenv('CI', 'false').lower() == 'true'
    is_gha = os.getenv('GITHUB_ACTIONS', 'false').lower() == 'true'
    
    print(f"ğŸ”‘ API Key: {'è®¾ç½®' if api_key else 'æœªè®¾ç½®'}")
    print(f"ğŸŒ Base URL: {base_url}")
    print(f"ğŸ—ï¸ CIç¯å¢ƒ: {'æ˜¯' if is_ci else 'å¦'}")
    print(f"ğŸ™ GitHub Actions: {'æ˜¯' if is_gha else 'å¦'}")
    
    if not api_key:
        print("âŒ é”™è¯¯: WANDB_API_KEY æœªè®¾ç½®")
        return False
    
    try:
        # åˆå§‹åŒ–W&B
        print("\n--- åˆå§‹åŒ–æç«¯Pipelineè¿è¡Œ ---")
        run = wandb.init(
            project="extreme-pipeline-test",
            name=f"{'gha' if is_gha else 'local'}-extreme-{int(time.time())}",
            tags=["extreme-test", "pipeline-stress", "artifact-debug"],
            config={
                "test_type": "extreme_pipeline",
                "environment": "gha" if is_gha else "local",
                "datasets": ["demo_day_2023_few_s1", "multiview_5_cams", "combined"],
                "model_count": 3,
                "total_files": "18+"
            }
        )
        print(f"âœ… æç«¯Pipelineè¿è¡Œåˆå§‹åŒ–: {run.name}")
        print(f"âœ… URL: {run.url}")
        
        # è®°å½•å¤æ‚çš„æŒ‡æ ‡
        print("\n--- è®°å½•å¤æ‚æŒ‡æ ‡ ---")
        for stage in ["preprocessing", "inference", "evaluation"]:
            wandb.log({
                f"{stage}_time": 45.2 + hash(stage) % 100,
                f"{stage}_memory_mb": 1024 + hash(stage) % 2048,
                f"{stage}_success": True
            })
        
        # è®°å½•MTMCæŒ‡æ ‡
        for dataset in ["demo_day_2023_few_s1", "multiview_5_cams", "combined"]:
            wandb.log({
                f"mtmc_{dataset}_hota": 65.63 + (hash(dataset) % 100) / 100,
                f"mtmc_{dataset}_deta": 68.05 + (hash(dataset) % 100) / 100,
                f"mtmc_{dataset}_assa": 63.423 + (hash(dataset) % 100) / 100
            })
        
        print("âœ… å¤æ‚æŒ‡æ ‡è®°å½•å®Œæˆ")
        
        # åˆ›å»ºå¤æ‚çš„pipelineè¾“å‡º
        print("\n--- åˆ›å»ºå¤æ‚Pipelineè¾“å‡º ---")
        temp_dir, files = create_complex_pipeline_outputs()
        
        total_size = sum(f.stat().st_size for f in files)
        print(f"âœ… åˆ›å»ºäº† {len(files)} ä¸ªå¤æ‚æ–‡ä»¶")
        print(f"âœ… æ€»å¤§å°: {total_size / (1024*1024):.1f} MB")
        
        # åˆ›å»ºå¤šä¸ªartifacts
        print("\n--- åˆ›å»ºå¤šä¸ªArtifacts ---")
        artifacts = create_multiple_artifacts(run, temp_dir, files)
        print(f"âœ… å‡†å¤‡äº† {len(artifacts)} ä¸ªä¸åŒç±»å‹çš„artifacts")
        
        # å¹¶å‘ä¸Šä¼ artifactsï¼ˆæ¨¡æ‹Ÿé«˜è´Ÿè½½ï¼‰
        print("\n--- å¹¶å‘ä¸Šä¼ Artifacts ---")
        uploaded_artifacts = []
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = [executor.submit(upload_artifact_with_retry, artifact_info) 
                      for artifact_info in artifacts]
            
            for future in futures:
                try:
                    logged_artifact = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    uploaded_artifacts.append(logged_artifact)
                except Exception as e:
                    print(f"âŒ å¹¶å‘ä¸Šä¼ å¤±è´¥: {e}")
        
        print(f"âœ… æˆåŠŸä¸Šä¼ äº† {len(uploaded_artifacts)} ä¸ªartifacts")
        
        # æç«¯ç­‰å¾…æ—¶é—´ï¼ˆæ¨¡æ‹ŸWovençš„å¤„ç†æ—¶é—´ï¼‰
        print("\n--- æç«¯ç­‰å¾…æ—¶é—´ ---")
        print("ğŸ”„ ç­‰å¾…60ç§’æ¨¡æ‹Ÿå¤æ‚çš„åå°å¤„ç†...")
        time.sleep(60)
        
        # éªŒè¯æ‰€æœ‰artifacts
        print("\n--- éªŒè¯æ‰€æœ‰Artifacts ---")
        api = wandb.Api()
        
        for artifact_type, original_artifact in artifacts:
            try:
                artifact_name = original_artifact.name
                retrieved_artifact = api.artifact(f"{run.entity}/{run.project}/{artifact_name}:latest")
                print(f"âœ… {artifact_type} APIéªŒè¯æˆåŠŸ: {retrieved_artifact.name}")
                print(f"  ğŸ“„ æ–‡ä»¶æ•°é‡: {len(retrieved_artifact.files())}")
                
                total_size = sum(file.size for file in retrieved_artifact.files())
                print(f"  ğŸ’¾ æ€»å¤§å°: {total_size / (1024*1024):.1f} MB")
                
            except Exception as e:
                print(f"âŒ {artifact_type} APIéªŒè¯å¤±è´¥: {e}")
                print("ğŸš¨ è¿™å¯èƒ½å°±æ˜¯Wovené‡åˆ°çš„é—®é¢˜ï¼")
        
        # æ¸…ç†
        import shutil
        shutil.rmtree(temp_dir)
        
        wandb.finish()
        print("\nâœ… æç«¯Pipelineæµ‹è¯•å®Œæˆ")
        
        print(f"\nğŸ”— è¯·æ£€æŸ¥W&B Dashboard: {run.url}")
        print("ğŸ“‹ é‡ç‚¹æ£€æŸ¥:")
        print("  1. æ‰€æœ‰artifactsæ˜¯å¦éƒ½æ˜¾ç¤ºåœ¨dashboardä¸­")
        print("  2. å¤§æ–‡ä»¶å’Œå¤æ‚ç»“æ„æ˜¯å¦æ­£ç¡®å¤„ç†")
        print("  3. å¹¶å‘ä¸Šä¼ æ˜¯å¦å¯¼è‡´ä»»ä½•é—®é¢˜")
        print("  4. å¯¹æ¯”æœ¬åœ°å’ŒGHAç¯å¢ƒçš„å·®å¼‚")
        
        return True
        
    except Exception as e:
        print(f"âŒ æç«¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)


